import time
import os
import gradio as gr
# 1. è°ƒæ•´æ–‡æ¡£åŠ è½½å™¨æ¨¡å—è·¯å¾„ï¼ˆè¿ç§»è‡³ langchain_communityï¼‰
from langchain_community.document_loaders import DirectoryLoader
# 2. è°ƒæ•´ ChatGLM LLM æ¨¡å—è·¯å¾„ï¼ˆè¿ç§»è‡³ langchain_communityï¼‰
from langchain_community.llms import ChatGLM
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import CharacterTextSplitter
# 3. è°ƒæ•´ HuggingFaceEmbeddings æ¨¡å—è·¯å¾„ï¼ˆè¿ç§»è‡³ langchain_communityï¼‰
from langchain_community.embeddings import HuggingFaceEmbeddings
# 4. è°ƒæ•´ Chroma å‘é‡å­˜å‚¨æ¨¡å—è·¯å¾„ï¼ˆè¿ç§»è‡³ langchain_communityï¼‰
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from neo4j import search

# è®¾ç½®åˆå§‹åŒ–æ˜¯å¦ä½¿ç”¨çŸ¥è¯†å›¾è°±æ•°æ®åº“æœç´¢ç›¸å…³çŸ¥è¯†ç‚¹
neo4j = False
if neo4j:
    neo4j_use = "çŸ¥è¯†å›¾è°±å·²å¯åŠ¨"
else:
    neo4j_use = "çŸ¥è¯†å›¾è°±å·²å…³é—­"


def load_documents(directory="document"):
    loader = DirectoryLoader(directory)
    documents = loader.load()
    text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0)
    split_docs = text_spliter.split_documents(documents)
    return split_docs


def load_embedding_model(local_model_path):
    encode_kwargs = {"normalize_embeddings": False}
    # ä½¿ç”¨cpu
    # model_kwargs = {"device": "cpu"}
    # cuda==11.8 pytorch==2.1.0 å¯ç”¨
    model_kwargs = {"device": "cuda:0"}
    return HuggingFaceEmbeddings(
        model_name=local_model_path,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    # 5. Chroma åˆå§‹åŒ–å‚æ•°å…¼å®¹ï¼ˆæ— éœ€ä¿®æ”¹ï¼Œlangchain_community ä¿æŒä¸€è‡´æ¥å£ï¼‰
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()
    return db


def add_text(history, text):
    history += [(text, None)]
    print(history)
    return history, gr.update(value="", interactive=False)


def add_file(history, file):
    global qa
    directory = os.path.dirname(file.name)
    documents = load_documents(directory)
    db = store_chroma(documents, embeddings)
    retriever = db.as_retriever()
    qa.retriever = retriever
    history = history + [((file.name,), None)]
    return history


def bot(history):
    global neo4j, neo4j_use
    if not history:
        return history
    message = history[-1][0]
    search_ans = False
    if neo4j:
        search_ans = search.search_relate(message)
    extra = ""
    if search_ans:
        extra = "\n\n-------------------------------------------\n\nä»¥ä¸‹æ˜¯æ ¹æ®ä½ çš„æé—®æ¨èçš„çŸ¥è¯†ç‚¹:\n" + search_ans
    if isinstance(message, tuple):
        response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ï¼"
    elif (message == "æ‰“å¼€çŸ¥è¯†å›¾è°±") or (message == "å…³é—­çŸ¥è¯†å›¾è°±"):
        response = neo4j_use
    else:
        # 6. RetrievalQA è°ƒç”¨æ–¹å¼å…¼å®¹ï¼ˆæ— éœ€ä¿®æ”¹ï¼Œä¿æŒåŸæœ‰æ¥å£ï¼‰
        response = qa({"query": message})['result']
        response += extra
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.01)
        yield history


def btn_neo4j_click(history):
    global neo4j_use, neo4j
    if neo4j_use == "çŸ¥è¯†å›¾è°±å·²å…³é—­":
        neo4j = True
        neo4j = search.connect_neo4j(neo4j)
    else:
        neo4j = False
        neo4j = search.connect_neo4j(neo4j)
    if neo4j:
        neo4j_use = "çŸ¥è¯†å›¾è°±å·²å¯åŠ¨"
        neo4j__use = "æ‰“å¼€çŸ¥è¯†å›¾è°±"
    else:
        neo4j_use = "çŸ¥è¯†å›¾è°±å·²å…³é—­"
        neo4j__use = "å…³é—­çŸ¥è¯†å›¾è°±"
    btn_neo4j.value = neo4j_use
    print("çŸ¥è¯†å›¾è°±çŠ¶æ€:", neo4j_use)
    history += [(neo4j__use, None)]
    return history


if __name__ == "__main__":
    # åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹
    embeddings = load_embedding_model(local_model_path='text2vec-base-chinese')
    # åŠ è½½å‘é‡æ•°æ®åº“
    if not os.path.exists('VectorStore'):
        documents = load_documents()
        db = store_chroma(documents, embeddings)
    else:
        # 7. Chroma åŠ è½½å‚æ•°å…¼å®¹ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
        db = Chroma(persist_directory='VectorStore', embedding_function=embeddings)
    # ä½¿ç”¨æœ¬åœ°apiæä¾›çš„å¤§æ¨¡å‹æœåŠ¡
    llm = ChatGLM(
        endpoint_url='http://127.0.0.1:8000',
        max_token=80000,
        top_p=0.9
    )
    # åˆ›å»ºqaé—®ç­”é“¾
    QA = PromptTemplate.from_template(
        """æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰å†…å®¹å›ç­”é—®é¢˜ã€‚
    å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±å›ç­”ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚
    ç­”æ¡ˆæœ€å¤š400ä¸ªå­—

    {context}

    é—®é¢˜ï¼š{question}

    """
    )
    retriever = db.as_retriever()
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        verbose=True,
        chain_type_kwargs={"prompt": QA}
    )
    # è®¾ç½®å‰ç«¯äº¤äº’é¡µé¢ï¼ˆä¿æŒä¸å˜ï¼‰
    with gr.Blocks() as demo:
        chatbot = gr.Chatbot(
            [],
            elem_id="AIåŠ©æ‰‹",
            bubble_full_width=False,
            avatar_images=(None, (os.path.join(os.path.dirname(__file__), "bot.jpg"))),
        )
        with gr.Row():
            query = gr.Textbox(
                scale=4,
                show_label=False,
                placeholder="è¾“å…¥é—®é¢˜å¹¶æŒ‰ä¸‹å›è½¦é”®æäº¤",
                container=False,
            )
            btn = gr.UploadButton("ğŸ“ä¸Šä¼ å¤–æŒ‚æ•°æ®åº“", file_types=['txt'])
            btn_neo4j = gr.Button(value="å¼€å…³çŸ¥è¯†å›¾è°±")
        neo4j_msg = btn_neo4j.click(btn_neo4j_click, [chatbot], [chatbot], queue=False).then(
            bot, chatbot, chatbot
        )
        query_msg = query.submit(add_text, [chatbot, query], [chatbot, query], queue=False).then(
            bot, chatbot, chatbot
        )
        query_msg.then(lambda: gr.update(interactive=True), None, [query], queue=False)
        file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
            bot, chatbot, chatbot
        )

    demo.queue()
    demo.launch()
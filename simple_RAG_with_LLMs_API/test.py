import time
import os
import gradio as gr
from langchain_community.document_loaders import DirectoryLoader
from langchain_community.llms import ChatGLM
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from neo4j import search

neo4j = False
neo4j_use = "çŸ¥è¯†å›¾è°±å·²å¯åŠ¨" if neo4j else "çŸ¥è¯†å›¾è°±å·²å…³é—­"


def load_documents(directory="document"):
    loader = DirectoryLoader(directory)
    documents = loader.load()
    text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0)
    split_docs = text_spliter.split_documents(documents)
    return split_docs


def load_embedding_model(local_model_path):
    encode_kwargs = {"normalize_embeddings": False}
    model_kwargs = {"device": "cpu"}
    return HuggingFaceEmbeddings(
        model_name=local_model_path,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()
    return db


def add_text(history, text):
    history += [(text, None)]
    return history, gr.update(value="", interactive=False)


def add_file(history, file):
    global qa_chain, retriever
    directory = os.path.dirname(file.name)
    documents = load_documents(directory)
    db = store_chroma(documents, embeddings)
    retriever = db.as_retriever()
    qa_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | QA_PROMPT
            | llm
            | StrOutputParser()
    )
    history = history + [((file.name,), None)]
    return history


### ä¿®å¤ï¼šbotå‡½æ•°è¿”å›ä¸¤ä¸ªå€¼
def bot(history):
    global neo4j, neo4j_use, qa_chain
    if not history:
        yield history, gr.update(interactive=True)  # åˆå§‹è¿”å›ä¸¤ä¸ªå€¼
        return
    message = history[-1][0]
    search_ans = False
    if neo4j:
        search_ans = search.search_relate(message)
    extra = ""
    if search_ans:
        extra = "\n\n-------------------------------------------\n\nä»¥ä¸‹æ˜¯æ ¹æ®ä½ çš„æé—®æ¨èçš„çŸ¥è¯†ç‚¹:\n" + search_ans
    if isinstance(message, tuple):
        response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ï¼"
    elif (message == "æ‰“å¼€çŸ¥è¯†å›¾è°±") or (message == "å…³é—­çŸ¥è¯†å›¾è°±"):
        response = neo4j_use
    else:
        response = qa_chain.invoke(message)
        response += extra
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.01)
        ### æ¯æ¬¡yieldè¿”å›ä¸¤ä¸ªå€¼ï¼šchatbot + æ¢å¤è¾“å…¥æ¡†äº¤äº’
        yield history, gr.update(interactive=True)


def btn_neo4j_click(history):
    global neo4j_use, neo4j
    if neo4j_use == "çŸ¥è¯†å›¾è°±å·²å…³é—­":
        neo4j = True
        neo4j = search.connect_neo4j(neo4j)
    else:
        neo4j = False
        neo4j = search.connect_neo4j(neo4j)
    if neo4j:
        neo4j_use = "çŸ¥è¯†å›¾è°±å·²å¯åŠ¨"
        neo4j__use = "æ‰“å¼€çŸ¥è¯†å›¾è°±"
    else:
        neo4j_use = "çŸ¥è¯†å›¾è°±å·²å…³é—­"
        neo4j__use = "å…³é—­çŸ¥è¯†å›¾è°±"
    btn_neo4j.value = neo4j_use
    print("çŸ¥è¯†å›¾è°±çŠ¶æ€:", neo4j_use)
    history += [(neo4j__use, None)]
    return history


def format_docs(docs):
    return "\n\n".join([doc.page_content for doc in docs])


QA_PROMPT = PromptTemplate.from_template(
    """æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰å†…å®¹å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±å›ç­”ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚
ç­”æ¡ˆæœ€å¤š400ä¸ªå­—

ä½†æ˜¯å¦‚æœé—®åˆ°ä¸‹é¢çš„å†…å®¹ï¼Œå°±å›å¤å¯¹åº”çš„é“¾æ¥ + å¯¹è¯¥ç§‘ç›®çš„ä»‹ç» + å¤ä¹ æ–¹æ³•ï¼Œæ¯éƒ¨åˆ†æ¢è¡Œå¤„ç†ï¼Œä¸€ç™¾å­—å·¦å³
ç§‘ç›®å’Œé“¾æ¥çš„å¯¹åº”å…³ç³»å¦‚ä¸‹ï¼š
å¤§å­¦ç‰©ç†ï¼šé“¾æ¥: https://pan.baidu.com/s/19B5uc8Mjr0SPuQZ0I9U8PA æå–ç : dm7a
æ“ä½œç³»ç»Ÿï¼šé“¾æ¥: https://pan.baidu.com/s/1ZYjE6Jk9Uf2c85Ya84KZXA æå–ç : pb3i
çº¿æ€§ä»£æ•°ï¼šé“¾æ¥: https://pan.baidu.com/s/1dfK6Z1He03LkhFy5nLz1zA æå–ç : thd9
è®¡ç®—æœºç»„æˆåŸç†ï¼šé“¾æ¥: https://pan.baidu.com/s/1a2f8uI42J3vOF-EWLmv9ow æå–ç : k8hw
è®¡ç®—æœºç½‘ç»œï¼šé“¾æ¥: https://pan.baidu.com/s/1ccrUng8ViMoIcZVhxEgJXA æå–ç : f14e

{context}

é—®é¢˜ï¼š{question}

"""
)

if __name__ == "__main__":
    embeddings = load_embedding_model(local_model_path=r'D:\simple_RAG_with_LLMs_API\text2vec-base-chinese')
    if not os.path.exists('VectorStore'):
        documents = load_documents()
        db = store_chroma(documents, embeddings)
    else:
        db = Chroma(persist_directory='VectorStore', embedding_function=embeddings)
    llm = ChatGLM(
        endpoint_url='http://127.0.0.1:8000',
        max_token=2048,
        top_p=0.9
    )
    retriever = db.as_retriever()
    qa_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | QA_PROMPT
            | llm
            | StrOutputParser()
    )
    with gr.Blocks(
            theme=gr.themes.Soft(
                primary_hue=gr.themes.Color(
                    name="custom_blue",
                    c50="#e3f2fd", c100="#bbdefb", c200="#90caf9",
                    c300="#64b5f6", c400="#42a5f5", c500="#2196f3",
                    c600="#1e88e5", c700="#1976d2", c800="#1565c0",
                    c900="#0d47a1", c950="#0a3d62"
                ),
                secondary_hue=gr.themes.Color(
                    name="custom_gray",
                    c50="#fafafa", c100="#f5f5f5", c200="#eeeeee",
                    c300="#e0e0e0", c400="#bdbdbd", c500="#616161",
                    c600="#424242", c700="#333333", c800="#212121",
                    c900="#000000", c950="#0a0a0a"
                ),
                neutral_hue=gr.themes.Color(
                    name="custom_light",
                    c50="#ffffff", c100="#f9f9f9", c200="#f0f0f0",
                    c300="#e6e6e6", c400="#dcdcdc", c500="#f5f5f5",
                    c600="#cccccc", c700="#b3b3b3", c800="#999999",
                    c900="#7f7f7f", c950="#666666"
                )
            ),
            css="""
        .gradio-container { max-width:1200px !important; margin:0 auto; padding:2rem; }
        body { background:linear-gradient(135deg, #f5f7fa 0%, #e4e9f2 100%); }
        #AIåŠ©æ‰‹ { height:600px !important; border-radius:12px; box-shadow:0 4px 12px rgba(0,0,0,0.1); }
        .gr-chatbot .user-bubble { background:#e3f2fd !important; color:#0d47a1 !important; border-radius:18px !important; padding:12px 18px !important; font-size:16px !important; }
        .gr-chatbot .bot-bubble { background:#f5f5f5 !important; color:#616161 !important; border-radius:18px !important; padding:12px 18px !important; font-size:16px !important; }
        .gr-chatbot .avatar { width:40px !important; height:40px !important; border-radius:50% !important; }
        .gr-textbox { border-radius:24px !important; padding:12px 20px !important; font-size:16px !important; border:1px solid #e0e0e0 !important; box-shadow:0 2px 6px rgba(0,0,0,0.05) !important; }
        .gr-textbox:focus { border-color:#2196f3 !important; outline:none !important; box-shadow:0 0 0 3px rgba(33,150,243,0.2) !important; }
        .gr-button { border-radius:24px !important; padding:8px 16px !important; font-size:16px !important; margin:0 4px !important; transition:all 0.3s ease !important; }
        .gr-button:hover { transform:translateY(-2px) !important; box-shadow:0 4px 8px rgba(0,0,0,0.1) !important; }
        .gr-upload-button { background:#42a5f5 !important; color:white !important; }
        .gr-upload-button:hover { background:#1e88e5 !important; }
        .gr-button[value*="å¼€å…³çŸ¥è¯†å›¾è°±"] { background:#616161 !important; color:white !important; }
        .gr-button[value*="å·²å¼€å¯"] { background:#2196f3 !important; }
        """
    ) as demo:
        neo4j_enabled = gr.State(False)
        gr.Markdown("# ğŸ¤– AIåŠ©æ‰‹ï¼ˆçŸ¥è¯†å›¾è°±å¢å¼ºç‰ˆï¼‰", elem_id="title")
        gr.Markdown("### è¾“å…¥é—®é¢˜æˆ–ä¸Šä¼ æ–‡ä»¶ï¼Œä½“éªŒæ™ºèƒ½äº¤äº’", elem_id="subtitle")

        chatbot = gr.Chatbot(
            [], elem_id="AIåŠ©æ‰‹", bubble_full_width=False,
            avatar_images=(None, os.path.join(os.path.dirname(__file__), "bot.jpg")),
        )

        with gr.Row(elem_id="input-row", variant="compact"):
            query = gr.Textbox(scale=5, show_label=False, placeholder="è¾“å…¥é—®é¢˜å¹¶æŒ‰ä¸‹å›è½¦é”®æäº¤", container=False,
                               interactive=True)
            btn_upload = gr.UploadButton("ğŸ“ ä¸Šä¼ å¤–æŒ‚æ•°æ®åº“", file_types=['txt'], elem_id="upload-btn")
            btn_neo4j = gr.Button(value="å¼€å…³çŸ¥è¯†å›¾è°±ï¼ˆæœªå¼€å¯ï¼‰", elem_id="neo4j-btn")

        btn_neo4j.click(fn=btn_neo4j_click, inputs=[chatbot, neo4j_enabled],
                        outputs=[chatbot, neo4j_enabled, btn_neo4j], show_progress=True)
        query.submit(fn=add_text, inputs=[chatbot, query], outputs=[chatbot, query]).then(fn=bot, inputs=chatbot,
                                                                                          outputs=[chatbot, query])
        btn_upload.upload(fn=add_file, inputs=[chatbot, btn_upload], outputs=chatbot, show_progress=True)

    demo.launch(debug=True, server_name="0.0.0.0", server_port=7860)
import time
import os
import gradio as gr
# 1. è°ƒæ•´æ–‡æ¡£åŠ è½½å™¨æ¨¡å—è·¯å¾„ï¼ˆè¿ç§»è‡³ langchain_communityï¼‰
from langchain_community.document_loaders import DirectoryLoader
# 2. è°ƒæ•´ ChatGLM LLM æ¨¡å—è·¯å¾„ï¼ˆè¿ç§»è‡³ langchain_communityï¼‰
from langchain_community.llms import ChatGLM
from langchain_core.prompts import PromptTemplate
from langchain_text_splitters import CharacterTextSplitter
# 3. è°ƒæ•´ HuggingFaceEmbeddings æ¨¡å—è·¯å¾„ï¼ˆè¿ç§»è‡³ langchain_communityï¼‰
from langchain_community.embeddings import HuggingFaceEmbeddings
# 4. è°ƒæ•´ Chroma å‘é‡å­˜å‚¨æ¨¡å—è·¯å¾„ï¼ˆè¿ç§»è‡³ langchain_communityï¼‰
from langchain_community.vectorstores import Chroma
# æ–°å¢ï¼šä½¿ç”¨ LangChain æ ¸å¿ƒç»„ä»¶æ›¿ä»£ RetrievalQAï¼ˆæ— éœ€ langchain.chainsï¼‰
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from neo4j import search  # çŸ¥è¯†å›¾è°±ç›¸å…³ï¼ˆä¿æŒä¸å˜ï¼‰

# è®¾ç½®åˆå§‹åŒ–æ˜¯å¦ä½¿ç”¨çŸ¥è¯†å›¾è°±æ•°æ®åº“æœç´¢ç›¸å…³çŸ¥è¯†ç‚¹
neo4j = False
if neo4j:
    neo4j_use = "çŸ¥è¯†å›¾è°±å·²å¯åŠ¨"
else:
    neo4j_use = "çŸ¥è¯†å›¾è°±å·²å…³é—­"


def load_documents(directory="document"):
    loader = DirectoryLoader(directory)
    documents = loader.load()
    text_spliter = CharacterTextSplitter(chunk_size=256, chunk_overlap=0)
    split_docs = text_spliter.split_documents(documents)
    return split_docs


def load_embedding_model(local_model_path):
    encode_kwargs = {"normalize_embeddings": False}
    # ä½¿ç”¨cpu
    # model_kwargs = {"device": "cpu"}
    # cuda==11.8 pytorch==2.1.0 å¯ç”¨
    model_kwargs = {"device": "cpu"}
    return HuggingFaceEmbeddings(
        model_name=local_model_path,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )


def store_chroma(docs, embeddings, persist_directory="VectorStore"):
    # 5. Chroma åˆå§‹åŒ–å‚æ•°å…¼å®¹ï¼ˆæ— éœ€ä¿®æ”¹ï¼Œlangchain_community ä¿æŒä¸€è‡´æ¥å£ï¼‰
    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)
    db.persist()
    return db


def add_text(history, text):
    history += [(text, None)]
    print(history)
    return history, gr.update(value="", interactive=False)


def add_file(history, file):
    global qa_chain, retriever  # ä¿®æ”¹ï¼šä½¿ç”¨å…¨å±€ qa_chain å’Œ retriever
    directory = os.path.dirname(file.name)
    documents = load_documents(directory)
    db = store_chroma(documents, embeddings)
    retriever = db.as_retriever()  # æ›´æ–°æ£€ç´¢å™¨
    # é‡æ–°æ„å»º QA é“¾ï¼ˆæ›¿æ¢åŸ qa.retriever = retrieverï¼‰
    qa_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | QA_PROMPT
        | llm
        | StrOutputParser()
    )
    history = history + [((file.name,), None)]
    return history


def bot(history):
    global neo4j, neo4j_use, qa_chain  # ä¿®æ”¹ï¼šä½¿ç”¨å…¨å±€ qa_chain
    if not history:
        return history
    message = history[-1][0]
    search_ans = False
    if neo4j:
        search_ans = search.search_relate(message)
    extra = ""
    if search_ans:
        extra = "\n\n-------------------------------------------\n\nä»¥ä¸‹æ˜¯æ ¹æ®ä½ çš„æé—®æ¨èçš„çŸ¥è¯†ç‚¹:\n" + search_ans
    if isinstance(message, tuple):
        response = "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ï¼"
    elif (message == "æ‰“å¼€çŸ¥è¯†å›¾è°±") or (message == "å…³é—­çŸ¥è¯†å›¾è°±"):
        response = neo4j_use
    else:
        # ä¿®æ”¹ï¼šä½¿ç”¨ qa_chain æ›¿ä»£åŸ qa({"query": message})['result']
        response = qa_chain.invoke(message)
        response += extra
    history[-1][1] = ""
    for character in response:
        history[-1][1] += character
        time.sleep(0.01)
        yield history


def btn_neo4j_click(history):
    global neo4j_use, neo4j
    if neo4j_use == "çŸ¥è¯†å›¾è°±å·²å…³é—­":
        neo4j = True
        neo4j = search.connect_neo4j(neo4j)
    else:
        neo4j = False
        neo4j = search.connect_neo4j(neo4j)
    if neo4j:
        neo4j_use = "çŸ¥è¯†å›¾è°±å·²å¯åŠ¨"
        neo4j__use = "æ‰“å¼€çŸ¥è¯†å›¾è°±"
    else:
        neo4j_use = "çŸ¥è¯†å›¾è°±å·²å…³é—­"
        neo4j__use = "å…³é—­çŸ¥è¯†å›¾è°±"
    btn_neo4j.value = neo4j_use
    print("çŸ¥è¯†å›¾è°±çŠ¶æ€:", neo4j_use)
    history += [(neo4j__use, None)]
    return history


# -------------------------- æ–°å¢ï¼šæ ¸å¿ƒæ›¿æ¢é€»è¾‘ --------------------------
def format_docs(docs):
    """å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ‹¼æ¥ä¸ºä¸Šä¸‹æ–‡æ–‡æœ¬"""
    return "\n\n".join([doc.page_content for doc in docs])

# å…¨å±€å˜é‡ï¼šå®šä¹‰ QA æç¤ºæ¨¡æ¿ï¼ˆåŸ QA å˜é‡é‡å‘½åä¸º QA_PROMPTï¼‰
QA_PROMPT = PromptTemplate.from_template(
    """æ ¹æ®ä¸‹é¢çš„ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰å†…å®¹å›ç­”é—®é¢˜ã€‚
å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±å›ç­”ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚
ç­”æ¡ˆæœ€å¤š400ä¸ªå­—

ä½†æ˜¯å¦‚æœé—®åˆ°ä¸‹é¢çš„å†…å®¹ï¼Œå°±å›å¤å¯¹åº”çš„é“¾æ¥ + å¯¹è¯¥ç§‘ç›®çš„ä»‹ç» + å¤ä¹ æ–¹æ³•ï¼Œæ¯éƒ¨åˆ†æ¢è¡Œå¤„ç†ï¼Œä¸€ç™¾å­—å·¦å³
ç§‘ç›®å’Œé“¾æ¥çš„å¯¹åº”å…³ç³»å¦‚ä¸‹ï¼š
å¤§å­¦ç‰©ç†ï¼šé“¾æ¥: https://pan.baidu.com/s/19B5uc8Mjr0SPuQZ0I9U8PA æå–ç : dm7a
æ“ä½œç³»ç»Ÿï¼šé“¾æ¥: https://pan.baidu.com/s/1ZYjE6Jk9Uf2c85Ya84KZXA æå–ç : pb3i
çº¿æ€§ä»£æ•°ï¼šé“¾æ¥: https://pan.baidu.com/s/1dfK6Z1He03LkhFy5nLz1zA æå–ç : thd9
è®¡ç®—æœºç»„æˆåŸç†ï¼šé“¾æ¥: https://pan.baidu.com/s/1a2f8uI42J3vOF-EWLmv9ow æå–ç : k8hw
è®¡ç®—æœºç½‘ç»œï¼šé“¾æ¥: https://pan.baidu.com/s/1ccrUng8ViMoIcZVhxEgJXA æå–ç : f14e

{context}

é—®é¢˜ï¼š{question}

"""
)
# ----------------------------------------------------------------------


if __name__ == "__main__":
    # åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹
    embeddings = load_embedding_model(local_model_path=r'D:\simple_RAG_with_LLMs_API\text2vec-base-chinese')
    # åŠ è½½å‘é‡æ•°æ®åº“
    if not os.path.exists('VectorStore'):
        documents = load_documents()
        db = store_chroma(documents, embeddings)
    else:
        # 7. Chroma åŠ è½½å‚æ•°å…¼å®¹ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
        db = Chroma(persist_directory='VectorStore', embedding_function=embeddings)
    # ä½¿ç”¨æœ¬åœ°apiæä¾›çš„å¤§æ¨¡å‹æœåŠ¡
    llm = ChatGLM(
        endpoint_url='http://127.0.0.1:8000',
        max_token=2048,
        top_p=0.9
    )
    # -------------------------- ä¿®æ”¹ï¼šæ„å»º QA é“¾ï¼ˆæ›¿ä»£ RetrievalQAï¼‰ --------------------------
    retriever = db.as_retriever()  # åˆ›å»ºæ£€ç´¢å™¨
    # æ„å»ºæ£€ç´¢-ç”Ÿæˆé“¾ï¼šæ£€ç´¢æ–‡æ¡£ â†’ æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ â†’ æ‹¼æ¥æç¤º â†’ LLMç”Ÿæˆ â†’ è¾“å‡ºè§£æ
    qa_chain = (
        {"context": retriever | format_docs,  # æ£€ç´¢å¹¶æ ¼å¼åŒ–æ–‡æ¡£
         "question": RunnablePassthrough()}  # ä¼ é€’ç”¨æˆ·é—®é¢˜
        | QA_PROMPT  # æ‹¼æ¥æç¤ºæ¨¡æ¿
        | llm  # è°ƒç”¨å¤§æ¨¡å‹
        | StrOutputParser()  # è§£æè¾“å‡ºä¸ºå­—ç¬¦ä¸²
    )
    # ---------------------------------------------------------------------------------------
    # è®¾ç½®å‰ç«¯äº¤äº’é¡µé¢ï¼ˆä¿æŒä¸å˜ï¼‰
    with gr.Blocks() as demo:
        chatbot = gr.Chatbot(
            [],
            elem_id="AIåŠ©æ‰‹",
            bubble_full_width=False,
            avatar_images=(None, (os.path.join(os.path.dirname(__file__), "bot.jpg"))),
        )
        with gr.Row():
            query = gr.Textbox(
                scale=4,
                show_label=False,
                placeholder="è¾“å…¥é—®é¢˜å¹¶æŒ‰ä¸‹å›è½¦é”®æäº¤",
                container=False,
            )
            btn = gr.UploadButton("ğŸ“ä¸Šä¼ å¤–æŒ‚æ•°æ®åº“", file_types=['txt'])
            btn_neo4j = gr.Button(value="å¼€å…³çŸ¥è¯†å›¾è°±")
        neo4j_msg = btn_neo4j.click(btn_neo4j_click, [chatbot], [chatbot], queue=False).then(
            bot, chatbot, chatbot
        )
        query_msg = query.submit(add_text, [chatbot, query], [chatbot, query], queue=False).then(
            bot, chatbot, chatbot
        )
        query_msg.then(lambda: gr.update(interactive=True), None, [query], queue=False)
        file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=False).then(
            bot, chatbot, chatbot
        )

    demo.queue()
    demo.launch()